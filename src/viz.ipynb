{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets info\n",
    "f = open(\"rsc/datasets.json\")\n",
    "datasets = json.load(f)\n",
    "f.close()\n",
    "\n",
    "d = \"german\" # FIXME: da levare una volta convertito in .py e prenderlo da linea di comand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"out/results/{}_exps.csv\".format(d), index_col=0)\n",
    "\n",
    "# parse lists\n",
    "colss = ['s_groups', 'means_f', 'means_nf']\n",
    "\n",
    "regex_pat = re.compile(r' +', flags=re.IGNORECASE)\n",
    "for c in colss:\n",
    "    results_df[c] = results_df[c].str.replace(regex_pat, \" \", regex=True)\n",
    "    results_df[c] = results_df[c].str.replace(\" \", \",\")   \n",
    "results_df[colss] = results_df[colss].applymap(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_new(ax, df, s, fm, metric, fd_mode, color=\"red\", m=None):\n",
    "    if fd_mode != 'D':\n",
    "        df = df[(df['sensitive']== s) & (df['fair_method']== fm)]\n",
    "        alphas = df['alpha']\n",
    "        metrics =  df[metric]\n",
    "        \n",
    "        if len(alphas) > 1:\n",
    "            ax.plot(alphas, metrics, label=\"FairDILCA-{}\".format(fd_mode), color=color)\n",
    "        else:\n",
    "            ax.axhline(y=metrics.tolist()[0], label=\"FairDILCA-{}\".format(fd_mode), color=color)\n",
    "        ax.set_xlabel(\"Alpha\")\n",
    "        ax.set_title(\"{} ({}) - {}\".format(d, s, fm))\n",
    "    else:\n",
    "        df = df[(df['fair_method'] == fm) & (df['method']==m) & (df['sensitive'] == s)]\n",
    "        metrics = df[metric].unique()\n",
    "        if len(metrics) > 0:\n",
    "            metrics = df[metric].unique().tolist()[0]\n",
    "        else: return\n",
    "        ax.axhline(y=metrics, color=color, label=\"FairDILCA-{}\".format(fd_mode))\n",
    "        \n",
    "def plot_f_nf_metric(ax, df, s, fm, metric, fd_mode, color=\"red\", m=None):\n",
    "    if fd_mode != \"D\":\n",
    "        df = df[(df['sensitive']== s) & (df['fair_method']== fm)]\n",
    "        alphas = df['alpha']\n",
    "        metric_f = df[metric + \"_f\"]\n",
    "        metric_nf = df[metric + \"_nf\"]\n",
    "        if len(alphas) > 1:\n",
    "            ax.plot(alphas, metric_f, label=\"FairDILCA-{} (fair)\".format(fd_mode), color=color)\n",
    "            ax.plot(alphas, metric_nf, label=\"FairDILCA-{} (non-fair)\".format(fd_mode), color=color, alpha=0.5, linestyle=\"--\")\n",
    "        else:\n",
    "            ax.axhline(y=metric_f.tolist()[0], label=\"FairDILCA-{} (fair)\".format(fd_mode), color=color)\n",
    "            ax.axhline(y=metric_nf.tolist()[0], label=\"FairDILCA-{} (non-fair)\".format(fd_mode), color=color, alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "        ax.set_title(\"{} ({}) - {}\".format(d, s, fm))       \n",
    "    else:\n",
    "        df = df[(df['fair_method'] == fm) & (df['method']==m) & (df['sensitive'] == s)]\n",
    "        if len(df[metric+'_f'].unique()) > 0:\n",
    "            metrics_f = df[metric+'_f'].unique().tolist()[0]\n",
    "            metrics_nf = df[metric+'_nf'].unique().tolist()[0]\n",
    "        else: return\n",
    "        ax.axhline(y=metrics_f, color=color, label=\"FairDILCA-{} (fair)\".format(fd_mode))\n",
    "        ax.axhline(y=metrics_nf, color=color, label=\"FairDILCA-{} (non fair)\".format(fd_mode), linestyle='--')\n",
    "        ax.set_title(\"{} ({}) - {}/{}\".format(d, s, fm,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"out/results/{}\".format(d)): os.mkdir(\"out/results/{}\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting per modalitÃ  e per metriche singole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson_objs','l1_dist_objs', 'pearson', 'time', 'f1', 'bcss']\n",
    "f_nf_metrics = ['bcss']\n",
    "group_metrics = ['means']\n",
    "modes = ['CD', 'C', 'D']\n",
    "modes_colors = [\"red\", \"orange\", \"green\"]\n",
    "\n",
    "sensitives = results_df['sensitive'].unique()\n",
    "fair_methods = ['FM', 'FRR1', 'FRR2']\n",
    "contexts_df_d = [('B', 'M'), ('B', 'RR'), ('FRR2', 'RR')]\n",
    "\n",
    "for metr in metrics:\n",
    "    fig, axs = plt.subplots(len(sensitives), len(fair_methods), figsize=(4.5*len(fair_methods), 4*len(sensitives)))\n",
    "\n",
    "    for x, mode in enumerate(modes):\n",
    "        mode_df = results_df[(results_df['mode'] == mode) & (results_df['empty_context']== False)]\n",
    "        \n",
    "        if mode != 'D':\n",
    "            for i, s in enumerate(sensitives):\n",
    "                for j, fm in enumerate(fair_methods):\n",
    "                    if len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "                    else: axs_temp = axs[j]\n",
    "                    if metr in f_nf_metrics:\n",
    "                        plot_f_nf_metric(axs_temp, mode_df, s, fm, metr, mode, color=modes_colors[x])\n",
    "                    else:\n",
    "                        plot_metric_new(axs_temp, mode_df, s, fm, metr, mode, color=modes_colors[x])\n",
    "        else:\n",
    "            for i, s in enumerate(sensitives):\n",
    "                for j, (fm, m) in enumerate(contexts_df_d):\n",
    "                    if len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "                    else: axs_temp = axs[j]\n",
    "                    if metr in f_nf_metrics:\n",
    "                        plot_f_nf_metric(axs_temp, mode_df, s, fm, metr, mode, color=modes_colors[x], m=m) \n",
    "                    else:\n",
    "                        plot_metric_new(axs_temp, mode_df, s, fm, metr, mode, color=modes_colors[x], m=m)\n",
    "    if len(sensitives) > 1: h1, l1 = axs[0,0].get_legend_handles_labels()\n",
    "    else: h1, l1 = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=len(algos.keys()))  \n",
    "    fig.suptitle(\"{} {}\".format(metr, d))    \n",
    "    fig.tight_layout()  \n",
    "    fig.savefig(\"out/results/{}/test_{}_{}.png\".format(d, d, metr), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_colors = ['orchid', \"deepskyblue\", \"yellowgreen\"]\n",
    "for gm in group_metrics:\n",
    "    for i, s in enumerate(sensitives):\n",
    "        for x, mode in enumerate(modes):\n",
    "            mode_df = results_df[results_df['mode'] == mode]\n",
    "            groups_s= results_df[results_df['sensitive']==s]['s_groups'].to_list()[0]\n",
    "            groups_s = [int(e) for e in groups_s]\n",
    "            fig, axs = plt.subplots(1, len(groups_s), figsize=(4.5*len(groups_s), 4))\n",
    "            \n",
    "            if mode != \"D\":\n",
    "                for j, fm in enumerate(fair_methods):\n",
    "                    df = mode_df[(mode_df['fair_method']==fm) & (mode_df['sensitive']==s)]\n",
    "                    metric_f = list(zip(*df[gm + \"_f\"].to_list()))\n",
    "                    metric_nf = list(zip(*df[gm + \"_nf\"].to_list()))\n",
    "                    majority_group = str(df['majority_group'].to_list()[0]).replace(\".0\", \"\")\n",
    "                    alphas = df['alpha']\n",
    "                    for g in range(len(metric_f)):\n",
    "                        if len(alphas) > 1:\n",
    "                            axs[g].plot(alphas, metric_f[g], label=gm + \" (Dilca Fair-{} {})\".format(mode, fm), color=methods_colors[j])\n",
    "                            axs[g].plot(alphas, metric_nf[g], label=gm + \" (Dilca NON Fair-{} {})\".format(mode, fm), color=methods_colors[j], alpha=0.5)\n",
    "                        else:\n",
    "                            axs[g].axhline(y=metric_f[g], label=gm + \" (Dilca Fair-{} {}/{})\".format(mode, fm, m), color=methods_colors[j])\n",
    "                            axs[g].axhline(y=metric_nf[g], label=gm + \" (Dilca NON Fair-{} {}/{})\".format(mode, fm, m), color=methods_colors[j], alpha=0.5, linestyle=\"--\")\n",
    "                        axs[g].set_xlabel('Alpha')\n",
    "                        axs[g].set_title(\"MaG {} - MiG {}\".format(majority_group, groups_s[g]))\n",
    "            else:\n",
    "                for j, (fm, m) in enumerate(contexts_df_d):\n",
    "                        df = mode_df[(mode_df['fair_method']==fm) & (mode_df['sensitive']==s) & (mode_df['method']==m)]\n",
    "                        metric_f = list(zip(*df[gm + \"_f\"].to_list()))\n",
    "                        metric_nf = list(zip(*df[gm + \"_nf\"].to_list()))\n",
    "                        if len(df['majority_group']) >0:\n",
    "                            majority_group = str(df['majority_group'].to_list()[0]).replace(\".0\", \"\")\n",
    "                        else: continue\n",
    "                        for g in range(len(metric_f)):\n",
    "                            axs[g].axhline(y=metric_f[g], label=gm + \" (Dilca Fair-{} {}/{})\".format(mode, fm, m), color=methods_colors[j])\n",
    "                            axs[g].axhline(y=metric_nf[g], label=gm + \" (Dilca NON Fair-{} {}/{})\".format(mode, fm, m), color=methods_colors[j], alpha=0.5, linestyle=\"--\")\n",
    "                            axs[g].set_xlabel('Alpha')\n",
    "                            axs[g].set_title(\"MaG {} - MiG {}\".format(majority_group, groups_s[g]))\n",
    "                    \n",
    "            h1, l1 = axs[0].get_legend_handles_labels()\n",
    "            fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)\n",
    "            fig.suptitle(\"{} DilcaFair-{}: {} ({})\".format(gm, mode, d, s), fontweight=\"bold\", y=1.05)\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(\"out/results/{}/test_{}_{}_{}_{}.png\".format(d, d, gm, s, mode), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for metr in metrics:\n",
    "    fig, axs = plt.subplots(len(sensitives), len(modes), figsize=(4.5*len(modes), 4*len(sensitives)))\n",
    "\n",
    "    for i, s in enumerate(sensitives):\n",
    "        for x, mode in enumerate(modes):\n",
    "            mode_df = results_df[results_df['mode'] == mode]\n",
    "            if len(sensitives) > 1: axs_temp = axs[i,x]\n",
    "            else: axs_temp = axs[x]\n",
    "            if mode != 'D':\n",
    "                for j, fm in enumerate(fair_methods):\n",
    "                    df = mode_df[(mode_df['sensitive']== s) & (mode_df['fair_method']== fm)]\n",
    "                    alphas = df['alpha']\n",
    "                    \n",
    "                    if metr not in f_nf_metrics:\n",
    "                        metrics_data =  df[metr]\n",
    "                        if len(alphas) > 1:\n",
    "                            axs_temp.plot(alphas, metrics_data, label=\"{}\".format(fm), color=methods_colors[j])\n",
    "                        else:\n",
    "                            axs_temp.axhline(y=metrics_data.tolist()[0], label=\"{}\".format(fm), color=methods_colors[j])\n",
    "                    else:\n",
    "                        metrics_data_f = df[metr+'_f']\n",
    "                        metrics_data_nf = df[metr+'_nf']\n",
    "                        if len(alphas) > 1:\n",
    "                            axs_temp.plot(alphas, metrics_data_f, label=\"{} (fair)\".format(fm), color=methods_colors[j])\n",
    "                            axs_temp.plot(alphas, metrics_data_nf, label=\"{} (non fair)\".format(fm), color=methods_colors[j], linestyle=\"--\")\n",
    "                        else:\n",
    "                            axs_temp.axhline(y=metrics_data_f.tolist()[0], label=\"{} (fair)\".format(fm), color=methods_colors[j])\n",
    "                            axs_temp.axhline(y=metrics_data_nf.tolist()[0], label=\"{} (non fair)\".format(fm), color=methods_colors[j], linestyle=\"--\")       \n",
    "            else:\n",
    "                for j, (fm, m) in enumerate(contexts_df_d):\n",
    "                    df = mode_df[(mode_df['fair_method']==fm) & (mode_df['sensitive']==s) & (mode_df['method']==m)]\n",
    "                    if metr not in f_nf_metrics:\n",
    "                        if len(df[metr].unique()) >0 :\n",
    "                            metrics_data = df[metr].unique().tolist()[0]\n",
    "                        else: continue\n",
    "                        axs_temp.axhline(y=metrics_data, color=methods_colors[j], label=\"{}/{}\".format(fm, m))\n",
    "                    else:\n",
    "                        if len(df[metr+\"_f\"].unique()) > 0:\n",
    "                            metrics_data_f = df[metr+\"_f\"].unique().tolist()[0]\n",
    "                            metrics_data_nf = df[metr+\"_nf\"].unique().tolist()[0]\n",
    "                        else: continue\n",
    "                        axs_temp.axhline(y=metrics_data_f, color=methods_colors[j], label=\"{}/{} (fair)\".format(fm, m))\n",
    "                        axs_temp.axhline(y=metrics_data_nf, color=methods_colors[j], label=\"{}/{} (non fair)\".format(fm, m), linestyle=\"--\")\n",
    "                        \n",
    "            axs_temp.set_xlabel(\"Alpha\")\n",
    "            axs_temp.set_title(\"{} ({}) - FairDILCA-{}\".format(d, s, mode))\n",
    "                \n",
    "    if len(sensitives) > 1: h1, l1 = axs[0,0].get_legend_handles_labels()\n",
    "    else: h1, l1 = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)  \n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(\"{} - {}\".format(d, metr), fontweight=\"bold\", y=1.05)\n",
    "    fig.savefig(\"out/results/{}/test_{}_methods_{}.png\".format(d, d, metr), bbox_inches='tight')\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"out/results/exps_{}_clustering.csv\".format(d), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[results_df['empty_context']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse lists\n",
    "colss = ['mfc_f', 'mfc_nf', 'balance_f', 'balance_nf', 'entropy_f', 'entropy_nf', \"groups\"]\n",
    "results_df['groups'] = results_df['groups'].str.replace(\" \", \",\")   \n",
    "\n",
    "regex_pat = re.compile(r' +', flags=re.IGNORECASE)\n",
    "for c in colss:\n",
    "    results_df[c] = results_df[c].str.replace(regex_pat, \" \", regex=True)\n",
    "    #results_df[c] = results_df[c].str.replace(\" \", \",\")   \n",
    "results_df[colss] = results_df[colss].applymap(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_min(arr):\n",
    "    return min(min(arr))\n",
    "\n",
    "def max_max(arr):\n",
    "    return max(max(arr))\n",
    "\n",
    "def mean_array(list):\n",
    "    return np.array(list).mean(axis=0)\n",
    "\n",
    "def min_array(list):\n",
    "    return np.array(list).min(axis=0)\n",
    "\n",
    "def max_array(list):\n",
    "    return np.array(list).max(axis=0)\n",
    "\n",
    "# results_df['mfc_sv_f'] = results_df['mfc_f'].apply(max_max)\n",
    "# results_df['mfc_sv_nf'] = results_df['mfc_nf'].apply(max_max)\n",
    "# results_df['balance_sv_f'] = results_df['balance_f'].apply(min_min)\n",
    "# results_df['balance_sv_nf'] = results_df['balance_nf'].apply(min_min)\n",
    "# results_df['entropy_sv_f'] = results_df['entropy_f'].apply(max)\n",
    "# results_df['entropy_sv_nf'] = results_df['entropy_nf'].apply(max)\n",
    "\n",
    "results_df['balance_f_mean'] = results_df['balance_f'].apply(mean_array)\n",
    "# results_df['balance_f_min'] = results_df['balance_f'].apply(min_array)\n",
    "# results_df['balance_f_max'] = results_df['balance_f'].apply(max_array)\n",
    "\n",
    "results_df['balance_nf_mean'] = results_df['balance_nf'].apply(mean_array)\n",
    "# results_df['balance_nf_min'] = results_df['balance_nf'].apply(min_array)\n",
    "# results_df['balance_nf_max'] = results_df['balance_nf'].apply(max_array)\n",
    "\n",
    "results_df['mfc_f_mean'] = results_df['mfc_f'].apply(mean_array)\n",
    "# results_df['mfc_min'] = results_df['mfc_f'].apply(min_array)\n",
    "# results_df['mfc_max'] = results_df['mfc_f'].apply(max_array)\n",
    "\n",
    "results_df['mfc_nf_mean'] = results_df['mfc_nf'].apply(mean_array)\n",
    "# results_df['mfc_nf_min'] = results_df['mfc_nf'].apply(min_array)\n",
    "# results_df['mfc_nf_max'] = results_df['mfc_nf'].apply(max_array)\n",
    "\n",
    "#fm_gender_df['balance_f_mean'] = fm_gender_df['balance_f'].apply(mean_array)\n",
    "#np.array(fm_gender_df['balance_f'][1]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_types_colors = ['orchid', \"deepskyblue\", \"yellowgreen\"]\n",
    "modes = ['C', 'CD']\n",
    "f_nf_metrics = ['ari', 'nmi', 'mean_silhouette']\n",
    "clustering_types = results_df['clustering_type'].unique()\n",
    "sensitives = results_df['sensitive'].unique()\n",
    "fair_methods = ['FM', 'FRR1', 'FRR2']\n",
    "methods = results_df['method'].unique()\n",
    "\n",
    "for metr in f_nf_metrics:\n",
    "    for ct, clust_type in enumerate(clustering_types):\n",
    "        fig, axs = plt.subplots(len(sensitives), len(fair_methods), figsize=(4.5*len(fair_methods), 4*len(sensitives)))\n",
    "        for x, mode in enumerate(modes):\n",
    "            mode_df = results_df[results_df['mode'] == mode]\n",
    "            for i, s in enumerate(sensitives):\n",
    "                for j, fm in enumerate(fair_methods):\n",
    "                    if len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "                    else: axs_temp = axs[j]\n",
    "                    \n",
    "                    df = mode_df[(mode_df['sensitive']== s) & (mode_df['fair_method']== fm) & (mode_df['clustering_type'] == clust_type)]\n",
    "                    alphas = df['alpha']\n",
    "                    \n",
    "                    metrics_data_f = df[metr+'_f']\n",
    "                    metrics_data_nf = df[metr+'_nf']\n",
    "                    \n",
    "                    if len(alphas) > 1:\n",
    "                        axs_temp.plot(alphas, metrics_data_f, label=\"{} (fair)\".format(mode), color=clustering_types_colors[x])\n",
    "                        axs_temp.plot(alphas, metrics_data_nf, label=\"{} (non fair)\".format(mode), color=clustering_types_colors[x], linestyle=\"--\", alpha=0.5)\n",
    "                    else:\n",
    "                        axs_temp.axhline(y=metrics_data_f.tolist()[0], color=clustering_types_colors[x], label=\"{} (non fair)\".format(mode))\n",
    "                        axs_temp.axhline(y=metrics_data_nf.tolist()[0], color=clustering_types_colors[x], label=\"{} (non fair)\".format(mode), linestyle=\"--\", alpha=0.5)\n",
    "                            \n",
    "                    axs_temp.set_xlabel(\"Alpha\")\n",
    "                    axs_temp.set_title(\"{} ({}) - {}\".format(d, s, fm))\n",
    "                \n",
    "        if len(sensitives) > 1: h1, l1 = axs[0,0].get_legend_handles_labels()\n",
    "        else: h1, l1 = axs[0].get_legend_handles_labels()\n",
    "        fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)  \n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(\"{} - {} - {}\".format(d, metr, clust_type), fontweight=\"bold\", y=1.05)\n",
    "        fig.savefig(\"out/results/{}/test_clustering_{}_{}_{}.png\".format(d, clust_type, d, metr), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_metrics = ['mfc', 'balance', 'entropy']\n",
    "for metr in g_metrics:\n",
    "    for ct, clust_type in enumerate(clustering_types):\n",
    "        \n",
    "        for x, mode in enumerate(modes):\n",
    "            fig, axs = plt.subplots(len(sensitives), len(fair_methods), figsize=(10*len(fair_methods), 4*len(sensitives)))\n",
    "            mode_df = results_df[results_df['mode'] == mode]\n",
    "            for i, s in enumerate(sensitives):\n",
    "                for j, fm in enumerate(fair_methods):\n",
    "                    if len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "                    else: axs_temp = axs[j]\n",
    "                    \n",
    "                    df = mode_df[(mode_df['sensitive']== s) & (mode_df['fair_method']== fm) & (mode_df['clustering_type'] == clust_type)]\n",
    "                    alphas = df['alpha']\n",
    "                    \n",
    "                    if len(alphas) == 0: continue\n",
    "\n",
    "                    if metr == \"entropy\":\n",
    "                        metrics_data_f = df[metr + \"_f\"]\n",
    "                        metrics_data_nf = df[metr + \"_nf\"]\n",
    "                    else:\n",
    "                        metrics_data_f = df[metr + \"_f_mean\"]\n",
    "                        metrics_data_nf = df[metr + \"_nf_mean\"]\n",
    "\n",
    "                    metrics_group_data = [list(t) for t in zip(*metrics_data_f)]\n",
    "                    metrics_group_data_nf = [list(t) for t in zip(*metrics_data_nf)]\n",
    "\n",
    "                    metrics_group_data_nf = [gdnf[0] for gdnf in metrics_group_data_nf]\n",
    "\n",
    "                    metrics_group_data_final = []\n",
    "                    for ig in range(len(metrics_group_data_nf)):\n",
    "                        metrics_group_data_final.append([metrics_group_data_nf[ig]] + metrics_group_data[ig])\n",
    "\n",
    "                    groups = df['groups'].tolist()[0]\n",
    "\n",
    "                    w = 1.0\n",
    "                    num_x = len(alphas) + 1 \n",
    "                    num_groups = len(groups) #stats.shape[0]\n",
    "                    first_tick = int(math.ceil((num_groups*w/2)))\n",
    "                    gap = num_groups*w + 1\n",
    "                    x = np.array([first_tick + ix*gap for ix in range(num_x)])\n",
    "\n",
    "                    b = []\n",
    "                    for ig in range(num_groups):\n",
    "                        b.append(axs_temp.bar(x - (ig - num_groups/2 + 0.5)*w, \n",
    "                                metrics_group_data_final[ig], \n",
    "                                width=w, \n",
    "                                align='center', \n",
    "                                edgecolor = 'black', \n",
    "                                linewidth = 1.0,\n",
    "                                label = groups[ig] \n",
    "                                ))\n",
    "                            \n",
    "                    axs_temp.set_ylabel(\"Values\")\n",
    "                    axs_temp.set_xlabel('Alpha')\n",
    "                    axs_temp.set_title(\"{} ({}) - {}\".format(d, s, fm))\n",
    "                    axs_temp.set_xticks(x)\n",
    "                    axs_temp.set_xticklabels([\"NF\"] + [str(a) for a in alphas])\n",
    "                    #ax.set_xticklabels(alphas)\n",
    "                    \"\"\"for i in range(num_groups):\n",
    "                        ax.bar_label(b[i], \n",
    "                                    padding = 3,\n",
    "                                    color = \"white\",\n",
    "                                    label_type='center', \n",
    "                                    rotation = 'vertical')\"\"\"\n",
    "                    axs_temp.legend()\n",
    "            fig.tight_layout()\n",
    "            fig.suptitle(\"{} - {} - {} | FairDilca {}\".format(d, metr, clust_type, mode), fontweight=\"bold\", y=1.05)\n",
    "            fig.savefig(\"out/results/{}/test_clustering_{}_{}_{}_FairDilca-{}.png\".format(d, clust_type, d, metr, mode), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"out/results/exps_{}_knn.csv\".format(d), index_col=0)\n",
    "results_df = results_df[results_df['empty_context']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_types_colors = ['orchid', \"deepskyblue\", \"yellowgreen\"]\n",
    "modes = ['C', 'CD']\n",
    "f_nf_metrics = ['accuracy', 'f1_macro', 'mcc', 'demographic_parity_diff', 'demographic_parity_ratio', 'equalized_odds_diff', 'equalized_odds_ratio', 'equal_opportunity_diff', 'equal_opportunity_ratio']\n",
    "sensitives = results_df['sensitive'].unique()\n",
    "fair_methods = ['FM', 'FRR1', 'FRR2']\n",
    "methods = results_df['method'].unique()\n",
    "\n",
    "for metr in f_nf_metrics:\n",
    "\tfig, axs = plt.subplots(len(sensitives), len(fair_methods), figsize=(4.5*len(fair_methods), 4*len(sensitives)))\n",
    "\tfor x, mode in enumerate(modes):\n",
    "\t\tmode_df = results_df[results_df['mode'] == mode]\n",
    "\t\tfor i, s in enumerate(sensitives):\n",
    "\t\t\tfor j, fm in enumerate(fair_methods):\n",
    "\t\t\t\tif len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "\t\t\t\telse: axs_temp = axs[j]\n",
    "\t\t\t\t\n",
    "\t\t\t\tdf = mode_df[(mode_df['sensitive']== s) & (mode_df['fair_method']== fm)]\n",
    "\t\t\t\talphas = df['alpha']\n",
    "\t\t\t\t\n",
    "\t\t\t\tmetrics_data_f = df[metr+'_f']\n",
    "\t\t\t\tmetrics_data_nf = df[metr+'_nf']\n",
    "\t\t\t\t\n",
    "\t\t\t\tif len(alphas) > 1:\n",
    "\t\t\t\t\taxs_temp.plot(alphas, metrics_data_f, label=\"{} (fair)\".format(mode), color=clustering_types_colors[x])\n",
    "\t\t\t\t\taxs_temp.plot(alphas, metrics_data_nf, label=\"{} (non fair)\".format(mode), color=clustering_types_colors[x], linestyle=\"--\", alpha=0.5)\n",
    "\t\t\t\telif len(alphas) == 1:\n",
    "\t\t\t\t\taxs_temp.axhline(y=metrics_data_f.tolist()[0], color=clustering_types_colors[x], label=\"{} (non fair)\".format(mode))\n",
    "\t\t\t\t\taxs_temp.axhline(y=metrics_data_nf.tolist()[0], color=clustering_types_colors[x], label=\"{} (non fair)\".format(mode), linestyle=\"--\", alpha=0.5)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\taxs_temp.set_xlabel(\"Alpha\")\n",
    "\t\t\t\taxs_temp.set_title(\"{} ({}) - {}\".format(d, s, fm))\n",
    "\t\t\t\n",
    "\tif len(sensitives) > 1: h1, l1 = axs[0,0].get_legend_handles_labels()\n",
    "\telse: h1, l1 = axs[0].get_legend_handles_labels()\n",
    "\tfig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)  \n",
    "\tfig.tight_layout()\n",
    "\tfig.suptitle(\"{} - {}\".format(d, metr), fontweight=\"bold\", y=1.05)\n",
    "\tfig.savefig(\"out/results/{}/test_knn_{}_{}.png\".format(d, d, metr), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitors (CorrelationRemover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor = \"CorrelationRemover\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_competitors_df = pd.read_pickle(\"out/results/competitors/exps_{}_{}.csv\".format(d, competitor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exps\n",
    "exps_metrics = [\"pearson_objs\", \"l1_dist_objs\", \"f1\", \"bcss\"]\n",
    "vs_metrics = [\"bcss\"]\n",
    "exps_group_metrics = [\"means\"]\n",
    "fair_methods = ['FM', 'FRR1', 'FRR2']\n",
    "fd_methods = [\"blue\", \"purple\", \"green\"]\n",
    "c_dilca_methods = [\"red\", \"orange\", \"orange\"]\n",
    "\n",
    "sensitives = results_competitors_df['sensitive'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metr in exps_metrics:\n",
    "    fig, axs = plt.subplots(len(sensitives), len(fair_methods), figsize=(4.5*len(fair_methods), 4*len(sensitives)))\n",
    "    for i, s in enumerate(sensitives):\n",
    "        for j, fm in enumerate(fair_methods):\n",
    "            if len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "            else: axs_temp = axs[j]\n",
    "            df = results_competitors_df[(results_competitors_df['sensitive']== s) & (results_competitors_df['fair_method']== fm)]\n",
    "            alphas = df['alpha']\n",
    "            \n",
    "            if metr in vs_metrics:\n",
    "                metric_competitor = df[metr + \"_dilca_competitor\"]\n",
    "                metric_fair_dilca = df[metr + \"_fair_dilca\"]\n",
    "                \n",
    "                if len(alphas) > 1:\n",
    "                    axs_temp.plot(alphas, metric_competitor, label=\"DILCA on {}\".format(competitor), color=\"r\")\n",
    "                    axs_temp.plot(alphas, metric_fair_dilca, label=\"FairDILCA\", color=\"blue\")\n",
    "                else:\n",
    "                    axs_temp.axhline(y=metric_competitor.to_list()[0], label=\"DILCA on {}\".format(competitor), color=\"r\")\n",
    "                    axs_temp.axhline(y=metric_fair_dilca.tolist()[0], label=\"FairDILCA-{}\".format(fm), color=\"blue\")\n",
    "            else: \n",
    "                metrics =  df[metr]\n",
    "                if len(alphas) > 1:\n",
    "                    axs_temp.plot(alphas, metrics, label=\"DILCA on {} \".format(competitor), color=\"r\")\n",
    "                else:\n",
    "                    axs_temp.axhline(y=metrics.tolist()[0], label=\"DILCA on {} \".format(competitor), color=\"r\")\n",
    "            \n",
    "            axs_temp.set_xlabel(\"Alpha\")\n",
    "            axs_temp.set_title(\"{} ({}) - {}\".format(d, s, fm))\n",
    "            \n",
    "    if len(sensitives) > 1: h1, l1 = axs[0,0].get_legend_handles_labels()\n",
    "    else: h1, l1 = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)  \n",
    "    fig.suptitle(\"{} {}\".format(metr, d))    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"out/results/competitors/{}/test_{}_{}.png\".format(d, d, metr), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gm in exps_group_metrics:\n",
    "    for i, s in enumerate(sensitives):\n",
    "        groups_s= results_competitors_df[results_competitors_df['sensitive']==s]['s_groups'].to_list()[0]\n",
    "        groups_s = [int(e) for e in groups_s]\n",
    "        fig, axs = plt.subplots(1, len(groups_s), figsize=(4.5*len(groups_s), 4))\n",
    "        \n",
    "        for j, fm in enumerate(fair_methods):\n",
    "            df = results_competitors_df[(results_competitors_df['fair_method']==fm) & (results_competitors_df['sensitive']==s)]\n",
    "            metric_f = list(zip(*df[gm + \"_competitor\"].to_list()))\n",
    "            metric_nf = list(zip(*df[gm + \"_fair_dilca\"].to_list()))\n",
    "            majority_group = str(df['majority_group'].to_list()[0]).replace(\".0\", \"\")\n",
    "            alphas = df['alpha']\n",
    "            m = df['method'].unique()[0]\n",
    "            for g in range(len(metric_f)):\n",
    "                if len(alphas) > 1:\n",
    "\n",
    "                    axs[g].plot(alphas, metric_f[g], label=gm + \" DILCA ({}) on {}\".format(m, competitor), color=c_dilca_methods[j])\n",
    "                    axs[g].plot(alphas, metric_nf[g], label=gm + \" FairDILCA ({})\".format(fm), color=fd_methods[j])\n",
    "                else:\n",
    "                    if fm!=\"FRR2\":\n",
    "                        axs[g].axhline(y=metric_f[g], label=gm + \" DILCA ({}) on {}\".format(m, competitor), color=c_dilca_methods[j])\n",
    "                    axs[g].axhline(y=metric_nf[g], label=gm + \" FairDILCA\", color=fd_methods[j])\n",
    "                axs[g].set_xlabel('Alpha')\n",
    "                axs[g].set_title(\"MaG {} - MiG {}\".format(majority_group, groups_s[g]))\n",
    "                            \n",
    "        h1, l1 = axs[0].get_legend_handles_labels()\n",
    "        fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)\n",
    "        fig.suptitle(\"{} DilcaFair-{}:({})\".format(gm, d, s), fontweight=\"bold\", y=1.05)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\"out/results/competitors/{}/test_{}_{}_{}.png\".format(d, d, gm, s), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_competitors_df = pd.read_pickle(\"out/results/competitors/exps_clustering_{}_{}.csv\".format(d, competitor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_types_colors = ['orchid', \"deepskyblue\", \"yellowgreen\"]\n",
    "f_nf_metrics = ['ari', 'nmi', 'mean_silhouette']\n",
    "clustering_types = results_competitors_df['clustering_type'].unique()\n",
    "sensitives = results_competitors_df['sensitive'].unique()\n",
    "fair_methods = ['FM', 'FRR1', 'FRR2']\n",
    "\n",
    "methods = results_competitors_df['method'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metr in f_nf_metrics:\n",
    "    for ct, clust_type in enumerate(clustering_types):\n",
    "        fig, axs = plt.subplots(len(sensitives), len(fair_methods), figsize=(4.5*len(fair_methods), 4*len(sensitives)))\n",
    "        for i, s in enumerate(sensitives):\n",
    "            for j, fm in enumerate(fair_methods):\n",
    "                if len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "                else: axs_temp = axs[j]\n",
    "                \n",
    "                df = results_competitors_df[(results_competitors_df['sensitive']== s) & (results_competitors_df['fair_method']== fm) & (results_competitors_df['clustering_type'] == clust_type)]\n",
    "                alphas = df['alpha']\n",
    "                metric_competitor = df[metr + \"_competitor\"]\n",
    "                metric_fair_dilca = df[metr + \"_fd\"]\n",
    "                \n",
    "                if len(alphas) > 1:\n",
    "                    axs_temp.plot(alphas, metric_competitor, label=\"DILCA on {}\".format(competitor), color=\"r\")\n",
    "                    axs_temp.plot(alphas, metric_fair_dilca, label=\"FairDILCA\", color=\"blue\")\n",
    "                else:\n",
    "                    axs_temp.axhline(y=metric_competitor.to_list()[0], label=\"DILCA on {}\".format(competitor), color=\"r\")\n",
    "                    axs_temp.axhline(y=metric_fair_dilca.tolist()[0], label=\"FairDILCA-{}\".format(fm), color=\"blue\")\n",
    "                axs_temp.set_xlabel(\"Alpha\")\n",
    "                axs_temp.set_title(\"{} ({}) - {}\".format(d, s, fm))        \n",
    "        if len(sensitives) > 1: h1, l1 = axs[0,0].get_legend_handles_labels()\n",
    "        else: h1, l1 = axs[0].get_legend_handles_labels()\n",
    "        fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)  \n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(\"{} - {} - {}\".format(d, metr, clust_type), fontweight=\"bold\", y=1.05)\n",
    "        fig.savefig(\"out/results/competitors/{}/test_clustering_{}_{}_{}.png\".format(d, clust_type, d, metr), bbox_inches='tight')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_competitors_df[(results_competitors_df['sensitive']== \"gender\") & (results_competitors_df['fair_method']== \"FM\") & (results_competitors_df['clustering_type'] == \"Agglomerative\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean array\n",
    "def mean_array(list):\n",
    "    return np.array(list).mean(axis=0)\n",
    "\n",
    "results_competitors_df['balance_competitor_mean'] = results_competitors_df['balance_competitor'].apply(mean_array)\n",
    "\n",
    "results_competitors_df['balance_fd_mean'] = results_competitors_df['balance_fd'].apply(mean_array)\n",
    "\n",
    "results_competitors_df['mfc_competitor_mean'] = results_competitors_df['mfc_competitor'].apply(mean_array)\n",
    "\n",
    "results_competitors_df['mfc_fd_mean'] = results_competitors_df['mfc_fd'].apply(mean_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_metrics = ['mfc', 'balance', 'entropy']\n",
    "fair_methods = ['FM', 'FRR1']\n",
    "for metr in g_metrics:\n",
    "    for ct, clust_type in enumerate(clustering_types):\n",
    "        for i, s in enumerate(sensitives):\n",
    "            fig, axs = plt.subplots(2, len(fair_methods), figsize=(10*len(fair_methods), 4*len(sensitives)))\n",
    "            for j, fm in enumerate(fair_methods):    \n",
    "                df = results_competitors_df[(results_competitors_df['sensitive']== s) & (results_competitors_df['fair_method']== fm) & (results_competitors_df['clustering_type'] == clust_type)]\n",
    "                alphas = df['alpha']\n",
    "                \n",
    "                if len(alphas) == 0: continue\n",
    "\n",
    "                if metr == \"entropy\":\n",
    "                    metrics_data_f = df[metr + \"_competitor\"]\n",
    "                    metrics_data_nf = df[metr + \"_fd\"]\n",
    "                else:\n",
    "                    metrics_data_f = df[metr + \"_competitor_mean\"]\n",
    "                    metrics_data_nf = df[metr + \"_fd_mean\"]\n",
    "\n",
    "                metrics_group_data = [list(t) for t in zip(*metrics_data_f)]\n",
    "                metrics_group_data_nf = [list(t) for t in zip(*metrics_data_nf)]\n",
    "\n",
    "                groups = df['groups'].tolist()[0]\n",
    "\n",
    "                w = 1.0\n",
    "                num_x = len(alphas)\n",
    "                num_groups = len(groups) #stats.shape[0]\n",
    "                first_tick = int(math.ceil((num_groups*w/2)))\n",
    "                gap = num_groups*w + 1\n",
    "                x = np.array([first_tick + ix*gap for ix in range(num_x)])\n",
    "\n",
    "                b = []\n",
    "                for ig in range(num_groups):\n",
    "                    b.append(axs[0][j].bar(x - (ig - num_groups/2 + 0.5)*w, \n",
    "                            metrics_group_data[ig], \n",
    "                            width=w, \n",
    "                            align='center', \n",
    "                            edgecolor = 'black', \n",
    "                            linewidth = 1.0,\n",
    "                            label = groups[ig] \n",
    "                            ))\n",
    "                axs[0][j].set_ylabel(\"Values\")\n",
    "                axs[0][j].set_xlabel('Alpha')\n",
    "                axs[0][j].set_title(\"DILCA ({}) on {}\".format(fm, competitor))\n",
    "                axs[0][j].set_xticks(x)\n",
    "                axs[0][j].set_xticklabels([str(a) for a in alphas])\n",
    "                b = []\n",
    "                for ig in range(num_groups):\n",
    "                    b.append(axs[1][j].bar(x - (ig - num_groups/2 + 0.5)*w, \n",
    "                            metrics_group_data_nf[ig], \n",
    "                            width=w, \n",
    "                            align='center', \n",
    "                            edgecolor = 'black', \n",
    "                            linewidth = 1.0,\n",
    "                            label = groups[ig] \n",
    "                            ))\n",
    "                axs[1][j].set_ylabel(\"Values\")\n",
    "                axs[1][j].set_xlabel('Alpha')\n",
    "                axs[1][j].set_title(\"FairDILCA ({})\".format(fm))\n",
    "                axs[1][j].set_xticks(x)\n",
    "                axs[1][j].set_xticklabels([str(a) for a in alphas])\n",
    "                \n",
    "                #ax.set_xticklabels(alphas)\n",
    "                \"\"\"for i in range(num_groups):\n",
    "                    ax.bar_label(b[i], \n",
    "                                padding = 3,\n",
    "                                color = \"white\",\n",
    "                                label_type='center', \n",
    "                                rotation = 'vertical')\"\"\"\n",
    "                axs_temp.legend()\n",
    "            fig.tight_layout()\n",
    "            fig.suptitle(\"{} - {} ({}) - {} \".format(metr, d, s, clust_type), fontweight=\"bold\", y=1.05)\n",
    "            fig.savefig(\"out/results/competitors/{}/test_clustering_{}_{}_{}_{}.png\".format(d, clust_type, d, metr, s), bbox_inches='tight')\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_competitors_df = pd.read_pickle(\"out/results/competitors/exps_knn_{}_{}.csv\".format(d, competitor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_nf_metrics = ['accuracy', 'f1_macro', 'mcc', 'demographic_parity_diff', 'demographic_parity_ratio', 'equalized_odds_diff', 'equalized_odds_ratio', 'equal_opportunity_diff', 'equal_opportunity_ratio']\n",
    "sensitives = results_competitors_df['sensitive'].unique()\n",
    "fair_methods = ['FM', 'FRR1', 'FRR2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metr in f_nf_metrics:\n",
    "\tfig, axs = plt.subplots(len(sensitives), len(fair_methods), figsize=(4.5*len(fair_methods), 4*len(sensitives)))\n",
    "\tfor i, s in enumerate(sensitives):\n",
    "\t\tfor j, fm in enumerate(fair_methods):\n",
    "\t\t\tif len(sensitives) > 1: axs_temp = axs[i,j]\n",
    "\t\t\telse: axs_temp = axs[j]\n",
    "\t\t\t\n",
    "\t\t\tdf = results_competitors_df[(results_competitors_df['sensitive']== s) & (results_competitors_df['fair_method']== fm)]\n",
    "\t\t\talphas = df['alpha']\n",
    "\t\t\t\n",
    "\t\t\tmetrics_data_competitor = df[metr+'_competitor']\n",
    "\t\t\tmetrics_data_fd = df[metr+'_fd']\n",
    "\t\t\t\n",
    "\t\t\tif len(alphas) > 1:\n",
    "\t\t\t\taxs_temp.plot(alphas, metrics_data_competitor, label=\"DILCA on {}\".format(competitor), color=\"r\")\n",
    "\t\t\t\taxs_temp.plot(alphas, metrics_data_fd, label=\"FairDILCA\", color=\"b\")\n",
    "\t\t\telif len(alphas) == 1:\n",
    "\t\t\t\taxs_temp.axhline(y=metrics_data_competitor.tolist()[0], color=\"r\", label=\"DILCA on {}\".format(competitor))\n",
    "\t\t\t\taxs_temp.axhline(y=metrics_data_fd.tolist()[0], color=\"b\", label=\"FairDILCA\")\n",
    "\t\t\t\t\t\n",
    "\t\t\taxs_temp.set_xlabel(\"Alpha\")\n",
    "\t\t\taxs_temp.set_title(\"{} ({}) - {}\".format(d, s, fm))\n",
    "\t\t\t\n",
    "\tif len(sensitives) > 1: h1, l1 = axs[0,0].get_legend_handles_labels()\n",
    "\telse: h1, l1 = axs[0].get_legend_handles_labels()\n",
    "\tfig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=5)  \n",
    "\tfig.tight_layout()\n",
    "\tfig.suptitle(\"{} - {}\".format(d, metr), fontweight=\"bold\", y=1.05)\n",
    "\tfig.savefig(\"out/results/competitors/{}/test_knn_{}_{}.png\".format(d, d, metr), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"obesity\"\n",
    "filename = \"out/results/competitors/knn_{}_CorrelationRemover.csv\".format(d)\n",
    "\n",
    "results = pd.read_pickle(filename)\n",
    "results[:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"out/results/exps_{}_knn.csv\".format(d)\n",
    "results = pd.read_pickle(filename)\n",
    "results[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import load_dataset\n",
    "for k in datasets.keys():\n",
    "    dataset, X, Y = load_dataset(datasets[k], \"rsc/originals/\")\n",
    "    print(k, Y.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuovi grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: aumentare font label ed assi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    \"DILCA_M\": {\n",
    "        \"color\": \"black\",\n",
    "        \"linestyle\": \"--\",\n",
    "         \"marker\": \"o\",\n",
    "         \"label\": \"DILCA_M\"\n",
    "        },\n",
    "    \"DILCA_RR\": {\n",
    "        \"color\": \"pink\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"marker\": \"o\",\n",
    "        \"label\": \"DILCA_RR\"\n",
    "    },\n",
    "    \"FairDILCA_FM\": {\n",
    "        \"color\": \"red\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"marker\": \"s\",\n",
    "        \"label\": \"FairDILCA_M\"    \n",
    "    },\n",
    "    \"FairDILCA_FRR1\": {\n",
    "        \"color\": \"blue\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"marker\": \"s\",\n",
    "        \"label\": \"FairDILCA_RR\"    \n",
    "    },\n",
    "    \"FairDILCA_FRR2\": {\n",
    "        \"color\": \"orange\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"marker\": \"s\",\n",
    "        \"label\": \"FairDILCA_PL\"    \n",
    "    },\n",
    "    \"CorrelationRemover\": {\n",
    "        \"color\": \"gold\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"marker\": \"d\",\n",
    "        \"label\": \"CR\"   \n",
    "    },\n",
    "    \"NonFairOneHotEncoded\": {\n",
    "       \"color\": \"violet\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"marker\": \"d\",\n",
    "        \"label\": \"NFHE\"    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# load datasets info\n",
    "f = open(\"rsc/datasets.json\")\n",
    "datasets = json.load(f)\n",
    "f.close()\n",
    "\n",
    "d = \"obesity\"\n",
    "competitor = \"CorrelationRemover\"\n",
    "path_exps = \"out/results/\"\n",
    "path_competitors = path_exps + \"competitors/\"\n",
    "\n",
    "if not os.path.exists(path_exps): os.mkdir(path_exps)\n",
    "if not os.path.exists(path_competitors): os.mkdir(path_competitors)\n",
    "if not os.path.exists(path_competitors + \"{}\".format(d)): os.mkdir(path_competitors + \"{}\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: aggiornare codice per riflettere modifiche exps competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename_dilca_fair_dilca = path_exps + \"{}_exps.csv\".format(d)\n",
    "filename_dilca_competitor = path_competitors + \"exps_{}_{}.csv\".format(d, competitor)\n",
    "\n",
    "results_dilca_fair_dilca = pd.read_pickle(filename_dilca_fair_dilca)\n",
    "results_dilca_competitor = pd.read_pickle(filename_dilca_competitor)\n",
    "\n",
    "results_dilca_competitor = results_dilca_competitor = results_dilca_competitor.rename(columns={\"means_MaG-Migs_competitor\": \"means_MaG-MiGs_competitor\", \"means_MaG-Migs_dilca\": \"means_MaG-MiGs_dilca\"})\n",
    "\n",
    "sensitives = [s for s in results_dilca_fair_dilca['sensitive'].unique()]\n",
    "sensitives2 = [s for s in results_dilca_competitor['sensitive'].unique()]\n",
    "print(sensitives==sensitives2)\n",
    "\n",
    "metrics = [\"pearson_objs\"]\n",
    "contexts = [('M', 'FM'), ('RR', 'FRR1'), ('RR', 'FRR2')]\n",
    "\n",
    "for metric in metrics:\n",
    "    for s in sensitives:\n",
    "        fig, axs = plt.subplots()\n",
    "        # for DILCA based methods\n",
    "        for m, fm in contexts:\n",
    "            df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['mode']== 'CD') ]\n",
    "            alphas = df['alpha']\n",
    "            metric_data = df[metric]\n",
    "\n",
    "            if fm=='FRR2':\n",
    "                axs.axhline(y=metric_data.tolist()[0], label=algos[\"FairDILCA_{}\".format(fm)]['label'], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "            else:\n",
    "                axs.plot(alphas, metric_data, label=algos[\"FairDILCA_{}\".format(fm)]['label'], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "        # for competitors\n",
    "        competitor_context = ['M']\n",
    "        for m in competitor_context:\n",
    "            df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== m)]\n",
    "            alphas = results_dilca_fair_dilca['alpha'].unique()\n",
    "            metric_data = df[metric+\"_original\"]\n",
    "            \n",
    "            axs.plot(alphas, metric_data, label=algos[competitor][\"label\"], color=algos[competitor]['color'], linestyle=algos[competitor][\"linestyle\"], marker=algos[competitor][\"marker\"])\n",
    "        \n",
    "        axs.set_xlabel(\"Alpha\", fontsize=16)\n",
    "        axs.set_ylabel(metric.replace(\"_\", \" \").title(), fontsize=16)\n",
    "        h, l = axs.get_legend_handles_labels()\n",
    "        #fig.legend(h, l,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=len(algos.keys()))  \n",
    "        #fig.suptitle(\"{} ({}) - {}\".format(d, s, metric.replace(\"_\", \" \").title()))   \n",
    "        axs.tick_params(axis='x', labelsize=16)\n",
    "        axs.tick_params(axis='y', labelsize=16) \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\"out/results/competitors/{}/test_{}_{}_{}.png\".format(d, d, s, metric), bbox_inches='tight', dpi=300)\n",
    "        fig.savefig(\"out/results/competitors/{}/test_{}_{}_{}.eps\".format(d, d, s, metric), bbox_inches='tight', format=\"eps\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['means_MaG-MiGs', 'means_MaG-MiGs_nn']\n",
    "\n",
    "for metric in metrics:\n",
    "    for s in sensitives:\n",
    "        fig, axs = plt.subplots()\n",
    "        # for FairDILCA \n",
    "        for m, fm in contexts:\n",
    "            df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['mode']== 'CD')]\n",
    "            alphas = df['alpha']\n",
    "            metric_data = df[metric+'_f']\n",
    "\n",
    "            if fm=='FRR2':\n",
    "                axs.axhline(y=metric_data.tolist()[0], label=algos[\"FairDILCA_{}\".format(fm)][\"label\"], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "            else:\n",
    "                axs.plot(alphas, metric_data, label=algos[\"FairDILCA_{}\".format(fm)]['label'], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "        # for DILCA\n",
    "        for m, fm in contexts[:2]:\n",
    "            df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['mode']== 'CD')]\n",
    "            alphas = df['alpha']\n",
    "            metric_data = df[metric+'_nf']\n",
    "\n",
    "            axs.plot(alphas, metric_data, label=algos[\"DILCA_{}\".format(m)]['label'], color=algos[\"DILCA_{}\".format(m)]['color'], linestyle=algos[\"DILCA_{}\".format(m)][\"linestyle\"], marker=algos[\"DILCA_{}\".format(m)][\"marker\"])\n",
    "        # for Competitors\n",
    "        if metric != \"means_MaG-MiGs_nn\":\n",
    "            for m in ['M']:\n",
    "                df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== m)]\n",
    "                alphas = results_dilca_fair_dilca['alpha'].unique()\n",
    "                metric_data = df[metric+'_competitor']\n",
    "                axs.plot(alphas, metric_data, label=algos[competitor][\"label\"], color=algos[competitor]['color'], linestyle=algos[competitor][\"linestyle\"], marker=algos[competitor][\"marker\"])\n",
    "        axs.tick_params(axis='x', labelsize=16)\n",
    "        axs.tick_params(axis='y', labelsize=16) \n",
    "        axs.set_xlabel(\"Alpha\", fontsize=16)\n",
    "        axs.set_ylabel(metric.replace(\"_\", \" \").title(), fontsize=16)\n",
    "        h, l = axs.get_legend_handles_labels()\n",
    "        #fig.legend(h, l,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=len(algos.keys()))  \n",
    "        #fig.suptitle(\"{} ({}) - {}\".format(d, s, \"MaGvsRest\"))    \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\"out/results/competitors/{}/test_{}_{}_{}.png\".format(d, d, s, metric), bbox_inches='tight', dpi=300)\n",
    "        fig.savefig(\"out/results/competitors/{}/test_{}_{}_{}.eps\".format(d, d, s, metric), bbox_inches='tight', format=\"eps\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"means\", \"means_nn\"]\n",
    "for metric in metrics:\n",
    "    for s in sensitives:\n",
    "        groups = results_dilca_fair_dilca[results_dilca_fair_dilca['sensitive']==s]['s_groups'].to_list()[0]\n",
    "        fig, axs = plt.subplots(1, len(groups), figsize=(4.5*len(groups), 4))\n",
    "        # FairDILCA\n",
    "        for m, fm in contexts:\n",
    "            df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['mode']== 'CD')]\n",
    "            alphas = df['alpha']\n",
    "            majority_group = df['majority_group'].to_list()[0]\n",
    "            metric_fair_dilca = list(zip(*df[metric + \"_f\"].to_list()))\n",
    "            for g in range(len(metric_fair_dilca)):\n",
    "                if fm == \"FRR2\": axs[g].axhline(y=metric_fair_dilca[g], label=algos[\"FairDILCA_{}\".format(fm)]['label'], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)]['linestyle'], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "                else: axs[g].plot(alphas, metric_fair_dilca[g], label=algos[\"FairDILCA_{}\".format(fm)]['label'], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)]['linestyle'], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"]) \n",
    "                axs.tick_params(axis='x', labelsize=16)\n",
    "                axs.tick_params(axis='y', labelsize=16) \n",
    "                axs[g].set_xlabel('Alpha')\n",
    "                axs[g].set_ylabel(metric.replace(\"_\", \" \").title())\n",
    "                axs[g].set_title(\"MaG ({}) - MiG ({})\".format(majority_group, groups[g]))\n",
    "        # DILCA\n",
    "        for m in [\"M\", \"RR\"]:\n",
    "            df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['mode']== 'CD')]\n",
    "            alphas = df['alpha']\n",
    "            majority_group = df['majority_group'].to_list()[0]\n",
    "            metric_dilca = list(zip(*df[metric +  \"_nf\"].to_list()))\n",
    "            for g in range(len(metric_dilca)):\n",
    "                axs[g].plot(alphas, metric_dilca[g], label=algos[\"DILCA_{}\".format(m)]['label'], color=algos[\"DILCA_{}\".format(m)]['color'], marker=algos[\"DILCA_{}\".format(m)][\"marker\"], linestyle=algos[\"DILCA_{}\".format(m)]['linestyle']) \n",
    "        # Competitor\n",
    "        if metric != \"means_nn\":\n",
    "            for m in ['M']:\n",
    "                df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== m)]\n",
    "                alphas = results_dilca_fair_dilca['alpha'].unique()\n",
    "                metric_competitor = list(zip(*df[metric + \"_competitor\"].to_list()))\n",
    "                for g in range(len(metric_competitor)):\n",
    "                    axs[g].plot(alphas, metric_competitor[g], label=algos[competitor]['label'], color=algos[competitor]['color'], linestyle=algos[competitor][\"linestyle\"], marker=algos[competitor][\"marker\"])\n",
    "\n",
    "        h1, l1 = axs[0].get_legend_handles_labels()\n",
    "        #fig.legend(h1, l1,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=len(algos.keys()))\n",
    "        #fig.suptitle(\"{} ({}) - {}\".format(d, s, metric.replace(\"_\", \" \").title()))    \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\"out/results/competitors/{}/test_{}_{}_{}.png\".format(d, d, s, metric), bbox_inches='tight', dpi=300)\n",
    "        fig.savefig(\"out/results/competitors/{}/test_{}_{}_{}.eps\".format(d, d, s, metric), bbox_inches='tight', format=\"eps\", dpi=300)      \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_algos = {\n",
    "\t\"Agglomerative\": {\n",
    "\t\t\"label\": \"HCL\"\n",
    "    },\n",
    "\t\"HDBSCAN\": {\n",
    "\t\t\"label\": \"HDBSCAN\"\n",
    "    }\n",
    "\t#\"Spectral\" : {\n",
    "\t#\t\"label\": \"SC\"\n",
    "\t#}\n",
    "}\n",
    "\n",
    "clustering_types = [k for k in clust_algos.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dilca_fair_dilca = path_exps + \"exps_{}_clustering.csv\".format(d)\n",
    "filename_dilca_competitor = path_competitors + \"clustering_{}_{}.csv\".format(d, competitor)\n",
    "\n",
    "results_dilca_fair_dilca = pd.read_csv(filename_dilca_fair_dilca, index_col=0)\n",
    "results_dilca_competitor = pd.read_pickle(filename_dilca_competitor)\n",
    "\n",
    "sensitives = [s for s in results_dilca_fair_dilca['sensitive'].unique()]\n",
    "#sensitives2 = [s for s in results_dilca_competitor['sensitive'].unique()]\n",
    "#print(sensitives==sensitives2)\n",
    "\n",
    "# parse lists\n",
    "colss = ['mfc_f', 'mfc_nf', 'balance_f', 'balance_nf', 'entropy_f', 'entropy_nf', \"groups\"]\n",
    "results_dilca_fair_dilca['groups'] = results_dilca_fair_dilca['groups'].str.replace(\" \", \",\")   \n",
    "\n",
    "regex_pat = re.compile(r' +', flags=re.IGNORECASE)\n",
    "for c in colss:\n",
    "    results_dilca_fair_dilca[c] = results_dilca_fair_dilca[c].str.replace(regex_pat, \" \", regex=True)\n",
    "    #results_df[c] = results_df[c].str.replace(\" \", \",\")   \n",
    "results_dilca_fair_dilca[colss] = results_dilca_fair_dilca[colss].applymap(literal_eval)\n",
    "\n",
    "def min_min(arr):\n",
    "    return min(min(arr))\n",
    "\n",
    "def max_max(arr):\n",
    "    return max(max(arr))\n",
    "\n",
    "def mean_array(list):\n",
    "    return np.array(list).mean(axis=0)\n",
    "\n",
    "def min_array(list):\n",
    "    return np.array(list).min(axis=0)\n",
    "\n",
    "def max_array(list):\n",
    "    return np.array(list).max(axis=0)\n",
    "\n",
    "results_dilca_fair_dilca['mfc_sv_f'] = results_dilca_fair_dilca['mfc_f'].apply(max_max)\n",
    "results_dilca_fair_dilca['mfc_sv_nf'] = results_dilca_fair_dilca['mfc_nf'].apply(max_max)\n",
    "results_dilca_fair_dilca['balance_sv_f'] = results_dilca_fair_dilca['balance_f'].apply(min_min)\n",
    "results_dilca_fair_dilca['balance_sv_nf'] = results_dilca_fair_dilca['balance_nf'].apply(min_min)\n",
    "results_dilca_fair_dilca['entropy_sv_f'] = results_dilca_fair_dilca['entropy_f'].apply(max)\n",
    "results_dilca_fair_dilca['entropy_sv_nf'] = results_dilca_fair_dilca['entropy_nf'].apply(max)\n",
    "\n",
    "results_dilca_competitor['mfc_sv_competitor'] = results_dilca_competitor['mfc_competitor'].apply(max_max)\n",
    "results_dilca_competitor['balance_sv_competitor'] = results_dilca_competitor['balance_competitor'].apply(min_min)\n",
    "results_dilca_competitor['entropy_sv_competitor'] = results_dilca_competitor['entropy_competitor'].apply(max)\n",
    "results_dilca_competitor['mfc_sv_dilca'] = results_dilca_competitor['mfc_dilca'].apply(max_max)\n",
    "results_dilca_competitor['balance_sv_dilca'] = results_dilca_competitor['balance_dilca'].apply(min_min)\n",
    "results_dilca_competitor['entropy_sv_dilca'] = results_dilca_competitor['entropy_dilca'].apply(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metrics = ['ari', 'nmi', 'mean_silhouette']\n",
    "\n",
    "for metric in accuracy_metrics:\n",
    "    for s in sensitives:\n",
    "        for ct in clustering_types:\n",
    "            fig, axs = plt.subplots()\n",
    "            # for FairDILCA\n",
    "            for m, fm in contexts:\n",
    "                df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['mode']== 'CD') & (results_dilca_fair_dilca['clustering_type']== ct) ]\n",
    "                alphas = df['alpha']\n",
    "                metric_data = df[metric+'_f']\n",
    "\n",
    "                if fm=='FRR2':\n",
    "                    axs.axhline(y=metric_data.tolist()[0], label=algos[\"FairDILCA_{}\".format(fm)][\"label\"], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "                else:\n",
    "                    axs.plot(alphas, metric_data, label=algos[\"FairDILCA_{}\".format(fm)][\"label\"], color=algos[\"FairDILCA_{}\".format(fm)]['color'], linestyle=algos[\"FairDILCA_{}\".format(fm)][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "            # for DILCA\n",
    "            if metric != 'ari' and metric!= 'nmi':\n",
    "                for m, fm in contexts[:2]:\n",
    "                    df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['mode']== 'CD') & (results_dilca_fair_dilca['clustering_type']== ct)]\n",
    "                    alphas = df['alpha']\n",
    "                    metric_data = df[metric+'_nf']\n",
    "\n",
    "                    axs.plot(alphas, metric_data, label=algos[\"DILCA_{}\".format(m)]['label'], color=algos[\"DILCA_{}\".format(m)]['color'], linestyle=algos[\"DILCA_{}\".format(m)][\"linestyle\"], marker=algos[\"DILCA_{}\".format(m)][\"marker\"])\n",
    "                \n",
    "            # for Competitors\n",
    "            for m in ['M']:\n",
    "                df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== m) & (results_dilca_competitor['clustering_type']== ct)]\n",
    "                alphas = results_dilca_fair_dilca['alpha'].unique()\n",
    "                metric_data = df[metric+'_competitor']\n",
    "                axs.plot(alphas, metric_data, label=algos[competitor][\"label\"], color=algos[competitor][\"color\"], linestyle=algos[competitor][\"linestyle\"], marker=algos[competitor][\"marker\"])\n",
    "            \n",
    "            # df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== 'M')  & (results_dilca_competitor['clustering_type']== ct)]\n",
    "            # metric_data = df[metric+'_dilca'].tolist()[0]\n",
    "            # axs.axhline(y=metric_data, label=algos[\"NonFairOneHotEncoded\"][\"label\"], color=algos[\"NonFairOneHotEncoded\"][\"color\"], linestyle=algos[\"NonFairOneHotEncoded\"][\"linestyle\"], marker=algos[\"NonFairOneHotEncoded\"][\"marker\"])\n",
    "            axs.tick_params(axis='x', labelsize=16)\n",
    "            axs.tick_params(axis='y', labelsize=16) \n",
    "            axs.set_xlabel(\"Alpha\", fontsize=16)\n",
    "            axs.set_ylabel(metric.replace(\"_\", \" \").title(), fontsize=16)\n",
    "            h, l = axs.get_legend_handles_labels()\n",
    "            #fig.legend(h, l,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=len(algos.keys()))  \n",
    "            #fig.suptitle(\"{} ({}) - {} - {}\".format(d, s, metric.replace(\"_\", \" \").title(), clust_algos[ct][\"label\"]))    \n",
    "            fig.tight_layout()\n",
    "            fig.savefig(\"out/results/competitors/{}/test_clustering_{}_{}_{}_{}.png\".format(d, d, s, clust_algos[ct][\"label\"], metric), bbox_inches='tight', dpi=300)\n",
    "            fig.savefig(\"out/results/competitors/{}/test_clustering_{}_{}_{}_{}.eps\".format(d, d, s, clust_algos[ct][\"label\"], metric), bbox_inches='tight', format=\"eps\", dpi=300)\n",
    "\"\"\"            break\n",
    "        break\n",
    "    break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_metrics = [\"balance\", \"mfc\", \"entropy\"]\n",
    "\n",
    "for metric in fairness_metrics:\n",
    "    for s in sensitives:\n",
    "        for ct in clustering_types:\n",
    "            fig, axs = plt.subplots()\n",
    "            # DILCA\n",
    "            for m, fm in contexts[:2]:\n",
    "                df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['mode']== 'CD') & (results_dilca_fair_dilca['clustering_type']== ct)]\n",
    "                metric_dilca = df[metric + \"_sv_nf\"].unique()\n",
    "                axs.axhline(y=metric_dilca, label=algos[\"DILCA_{}\".format(m)]['label'], color=algos[\"DILCA_{}\".format(m)]['color'], linestyle=algos[\"DILCA_{}\".format(m)][\"linestyle\"], marker=algos[\"DILCA_{}\".format(m)][\"marker\"])\n",
    "            # FairDILCA\n",
    "            values_lists = []\n",
    "            for m, fm in contexts[:2]:\n",
    "                df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['mode']== 'CD') & (results_dilca_fair_dilca['clustering_type']== ct)]\n",
    "                metric_fair_dilca = df[metric + \"_sv_f\"]\n",
    "                \n",
    "                alphas = df[\"alpha\"]\n",
    "                values_lists.append(metric_fair_dilca.to_list())\n",
    "                \n",
    "            df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== \"FRR2\") & (results_dilca_fair_dilca['method']== \"RR\") & (results_dilca_fair_dilca['mode']== 'CD') & (results_dilca_fair_dilca['clustering_type']== ct)]\n",
    "            metric_fair_dilca = df[metric + \"_sv_f\"].unique()\n",
    "            axs.axhline(y=metric_dilca, label=algos[\"FairDILCA_{}\".format(\"FRR2\")]['label'], color=algos[\"FairDILCA_{}\".format(\"FRR2\")]['color'], linestyle=algos[\"FairDILCA_{}\".format(\"FRR2\")][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(\"FRR2\")][\"marker\"])\n",
    "            # Competitor\n",
    "            for m, fm in contexts[:1]:\n",
    "                df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== m)  & (results_dilca_competitor['clustering_type']== ct)]\n",
    "                metric_competitor = df[metric + \"_sv_competitor\"]\n",
    "                values_lists.append(metric_competitor.to_list())\n",
    "            df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== 'M')  & (results_dilca_competitor['clustering_type']== ct)]\n",
    "            metric_data = df[metric+'_sv_dilca'].tolist()[0]\n",
    "            axs.axhline(y=metric_data, label=algos[\"NonFairOneHotEncoded\"][\"label\"], color=algos[\"NonFairOneHotEncoded\"][\"color\"], linestyle=algos[\"NonFairOneHotEncoded\"][\"linestyle\"], marker=algos[\"NonFairOneHotEncoded\"][\"marker\"])\n",
    "            \n",
    "            w = 1.0 # width\n",
    "            num_x = len(alphas) # number of labels for x axis\n",
    "            num_groups = len(contexts) \n",
    "            first_tick = int(math.ceil((num_groups*w/2)))\n",
    "            gap = num_groups*w + 1\n",
    "            x = np.array([first_tick + ix*gap for ix in range(num_x)])\n",
    "\n",
    "            b = []\n",
    "            for ig in range(num_groups):\n",
    "                # per non prendere FRR2\n",
    "                if ig==2: index = competitor\n",
    "                else: index = [a for a in algos.keys()][2+ig]\n",
    "                b.append(axs.bar(x - (ig - num_groups/2 + 0.5)*w, \n",
    "                        values_lists[ig], \n",
    "                        width=w, \n",
    "                        align='center', \n",
    "                        edgecolor = 'black', \n",
    "                        linewidth = 1.0,\n",
    "                        label = algos[index][\"label\"],\n",
    "                        color = algos[index][\"color\"]\n",
    "                        ))\n",
    "            axs.tick_params(axis='x', labelsize=16)\n",
    "            axs.tick_params(axis='y', labelsize=16) \n",
    "            axs.set_ylabel(metric.replace(\"_\", \" \").title(), fontsize=16)\n",
    "            axs.set_xlabel('Alpha', fontsize=16)\n",
    "            axs.set_xticks(x)\n",
    "            \n",
    "            new = []\n",
    "            for i, a in enumerate(alphas):\n",
    "                if i%2==1:new.append(\"\")\n",
    "                else: new.append(a)\n",
    "            \n",
    "            axs.set_xticklabels([str(a) for a in new])\n",
    "            h, l = axs.get_legend_handles_labels()\n",
    "            #fig.legend(h, l,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=len(algos.keys()))  \n",
    "            #fig.suptitle(\"{} ({}) - {} - {}\".format(d, s, metric.replace(\"_\", \" \").title(), clust_algos[ct][\"label\"]))    \n",
    "            fig.tight_layout()\n",
    "            fig.savefig(\"out/results/competitors/{}/test_clustering_{}_{}_{}_{}.png\".format(d, d, s, clust_algos[ct][\"label\"], metric), bbox_inches='tight', dpi=300)\n",
    "            fig.savefig(\"out/results/competitors/{}/test_clustering_{}_{}_{}_{}.eps\".format(d, d, s, clust_algos[ct][\"label\"], metric), bbox_inches='tight', format=\"eps\", dpi=300)\n",
    "\"\"\"            break\n",
    "        break\n",
    "    break\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename_dilca_fair_dilca = path_exps + \"exps_{}_knn.csv\".format(d)\n",
    "filename_dilca_competitor = path_competitors + \"knn_{}_{}.csv\".format(d, competitor)\n",
    "\n",
    "results_dilca_fair_dilca = pd.read_pickle(filename_dilca_fair_dilca)\n",
    "results_dilca_competitor = pd.read_pickle(filename_dilca_competitor)\n",
    "results_dilca_competitor = results_dilca_competitor.rename(columns={\"demographic_ratio_competitor\": \"demographic_parity_ratio_competitor\", \"demographic_ratio_dilca\": \"demographic_parity_ratio_dilca\"})\n",
    "\n",
    "sensitives = [s for s in results_dilca_fair_dilca['sensitive'].unique()]\n",
    "sensitives2 = [s for s in results_dilca_competitor['sensitive'].unique()]\n",
    "ks = [s for s in results_dilca_fair_dilca['k'].unique()]\n",
    "ks2 = [s for s in results_dilca_fair_dilca['k'].unique()]\n",
    "print(sensitives==sensitives2)\n",
    "print(ks==ks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'f1_macro', 'mcc', 'demographic_parity_diff', 'demographic_parity_ratio', 'equalized_odds_diff', 'equalized_odds_ratio', 'equal_opportunity_diff', 'equal_opportunity_ratio']\n",
    "\n",
    "for k in ks:\n",
    "    for metric in metrics:\n",
    "        for s in sensitives:\n",
    "            fig, axs = plt.subplots()\n",
    "            # for DILCA based methods\n",
    "            for m, fm in contexts:\n",
    "                df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['mode']== 'CD') & (results_dilca_fair_dilca['k']== k)]\n",
    "                alphas = df['alpha']\n",
    "                metric_data = df[metric+'_f']\n",
    "\n",
    "                if fm=='FRR2':\n",
    "                    axs.axhline(y=metric_data.tolist()[0], label=algos[\"FairDILCA_FRR2\"][\"label\"], color=algos[\"FairDILCA_FRR2\"][\"color\"], linestyle=algos[\"FairDILCA_FRR2\"][\"linestyle\"], marker=algos[\"FairDILCA_FRR2\"][\"marker\"])\n",
    "                else:\n",
    "                    axs.plot(alphas, metric_data, label=algos[\"FairDILCA_{}\".format(fm)][\"label\"], color=algos[\"FairDILCA_{}\".format(fm)][\"color\"], linestyle=algos[\"FairDILCA_{}\".format(fm)][\"linestyle\"], marker=algos[\"FairDILCA_{}\".format(fm)][\"marker\"])\n",
    "            # for DILCA\n",
    "            for m, fm in contexts[:2]:\n",
    "                df = results_dilca_fair_dilca[(results_dilca_fair_dilca['sensitive']== s) & (results_dilca_fair_dilca['method']== m) & (results_dilca_fair_dilca['fair_method']== fm) & (results_dilca_fair_dilca['mode']== 'CD') & (results_dilca_fair_dilca['k']== k)]\n",
    "                alphas = df['alpha']\n",
    "                metric_data = df[metric+'_nf']\n",
    "\n",
    "                axs.plot(alphas, metric_data, label=algos[\"DILCA_{}\".format(m)][\"label\"], color=algos[\"DILCA_{}\".format(m)][\"color\"], linestyle=algos[\"DILCA_{}\".format(m)][\"linestyle\"], marker=algos[\"DILCA_{}\".format(m)][\"marker\"])  \n",
    "            # for Competitors\n",
    "            for m in ['M']:\n",
    "                df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== m)  & (results_dilca_competitor['k']== k)]\n",
    "                alphas = results_dilca_fair_dilca['alpha'].unique()\n",
    "                metric_data = df[metric+'_competitor']\n",
    "                axs.plot(alphas, metric_data, label=algos[competitor][\"label\"], color=algos[competitor]['color'], linestyle=algos[competitor][\"linestyle\"], marker=algos[competitor][\"marker\"])\n",
    "            df = results_dilca_competitor[(results_dilca_competitor['sensitive']== s) & (results_dilca_competitor['method']== 'M')  & (results_dilca_competitor['k']== k)]\n",
    "            metric_data = df[metric+'_dilca'].unique()\n",
    "            axs.axhline(y=metric_data.tolist()[0], label=algos[\"NonFairOneHotEncoded\"][\"label\"], color=algos[\"NonFairOneHotEncoded\"][\"color\"], linestyle=algos[\"NonFairOneHotEncoded\"][\"linestyle\"], marker=algos[\"NonFairOneHotEncoded\"][\"marker\"])\n",
    "            axs.tick_params(axis='x', labelsize=16)\n",
    "            axs.tick_params(axis='y', labelsize=16) \n",
    "            axs.set_xlabel(\"Alpha\", fontsize=16)\n",
    "            axs.set_ylabel(metric.replace(\"_\", \" \").title(), fontsize=16)\n",
    "            h, l = axs.get_legend_handles_labels()\n",
    "            #fig.legend(h, l,loc='upper center', bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=True, ncol=len(algos.keys()))  \n",
    "            #fig.suptitle(\"{} ({}) k={} - {}\".format(d, s, k, metric.replace(\"_\", \" \").title()))    \n",
    "            fig.tight_layout()\n",
    "            fig.savefig(\"out/results/competitors/{}/test_knn_{}_{}_k{}_{}.png\".format(d, d, s, k, metric), bbox_inches='tight', dpi=300)\n",
    "            fig.savefig(\"out/results/competitors/{}/test_knn_{}_{}_k{}_{}.eps\".format(d, d, s, k, metric), bbox_inches='tight', format=\"eps\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles,labels = axs.get_legend_handles_labels()\n",
    "empty_fig, axe = plt.subplots(figsize=(1, 1))\n",
    "axe.legend(handles, labels, loc=\"center\", framealpha=1, frameon=True, prop={'size': 22}, markerscale=2)\n",
    "axe.xaxis.set_visible(False)\n",
    "axe.yaxis.set_visible(False)\n",
    "for v in axe.spines.values():\n",
    "\tv.set_visible(False)\n",
    " \n",
    "empty_fig.savefig(\"out/results/competitors/{}/legend.png\".format(d), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles,labels = axs.get_legend_handles_labels()\n",
    "empty_fig, axe = plt.subplots(figsize=(1, 0))\n",
    "axe.legend(handles, labels, loc=\"center\", framealpha=1, frameon=True, bbox_to_anchor=(0.5, 0.00), fancybox=True, shadow=False, ncol=7, prop={'size': 22}, markerscale=2)\n",
    "axe.xaxis.set_visible(False)\n",
    "axe.yaxis.set_visible(False)\n",
    "for v in axe.spines.values():\n",
    "\tv.set_visible(False)\n",
    " \n",
    "empty_fig.savefig(\"out/results/competitors/{}/legend_.png\".format(d), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
